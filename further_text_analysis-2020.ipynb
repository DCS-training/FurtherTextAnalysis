{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "First we need to install the various Python libraries and resources that we will use in this workbook.\n",
    "\n",
    "(If you receive an error when running the following code block it is probably because the package is already installed, this is not a problem)\n",
    "\n",
    "**wordcloud** - used to creat a visualization\n",
    "\n",
    "**textblob** - for sentiment analysis\n",
    "\n",
    "**stopwords** - provides a list of common words to exclude from analysis\n",
    "\n",
    "**punkt** - helps with tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install sklearn\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download file via a URL\n",
    "\n",
    "We will use the following technique to download a CSV from the web and write to local CSV file.\n",
    "\n",
    "In this case the file is a CSV file containing all of Donald Trump's tweets. (The file can be found in the file list in the Noteable home tab. You can download from here for later use if desired)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://learn.edina.ac.uk/data/trump-tweet-archive.csv'  # download the file\n",
    "\n",
    "with urllib.request.urlopen(url)  as csv:  # assign the contents of the file to a variable (csv)\n",
    "\n",
    "    output = csv.read()\n",
    "\n",
    "with open('tweets.csv', 'wb') as new_file:  # create a new file and save the contents of 'csv' to this file\n",
    "    new_file.write(output)\n",
    "\n",
    "    print('New CSV file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect our new file\n",
    "\n",
    "We will use Python code to inspect the tweets file\n",
    "\n",
    "Open the file and print out the first N rows\n",
    "\n",
    "The number of rows (N) is set to 5 - change this to see more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = open('tweets.csv', 'r')  # open the csv data file\n",
    "reader = csv.reader(csv_file)\n",
    "    \n",
    "N = 5 # Change this number to view more/fewer lines\n",
    "    \n",
    "cnt = 0\n",
    "for row in reader:\n",
    "    if cnt < N:\n",
    "      print(row)\n",
    "      cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse the output\n",
    "\n",
    "We can also read the file in reverse order. This is useful in this case as we can view the latest tweets from Donald Trump. Again, you can change the variable 'N' to alter the number of tweets returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = open('tweets.csv', 'r')  # open the csv data file\n",
    "reader = csv.reader(csv_file)\n",
    "    \n",
    "N = 5 # Change this number to view more/fewer lines\n",
    "    \n",
    "cnt = 0\n",
    "for row in reversed(list(reader)):\n",
    "    if cnt < N:\n",
    "      print(row)\n",
    "      cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Tweets to text file\n",
    "\n",
    "For the next step we need to export the tweet text and write it to a text file. This simplifies the structure and makes much analysis easier.\n",
    "\n",
    "• We will ignore the tweet id and the date\n",
    "\n",
    "• We will exclude any retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = open('tweets.csv', 'r')  # open the csv data file\n",
    "next(csv_file, None)  # skip the header row\n",
    "reader = csv.reader(csv_file)\n",
    "\n",
    "# create/open output file\n",
    "text_file = open('tweets.txt','w')\n",
    "\n",
    "\n",
    "for row in reader:\n",
    "\n",
    "    tweet = row[2]\n",
    "    if ('RT @' not in tweet): # Exclude retweets\n",
    "       text_file.write(tweet + '\\n')\n",
    "\n",
    "csv_file.close()\n",
    "text_file.close()\n",
    "\n",
    "print(\"Tweets written to 'tweets.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "an N-gram is simply a sequence of N words. For instance:\n",
    "\n",
    "• San Francisco (is a 2-gram)\n",
    "\n",
    "• The Three Musketeers (is a 3-gram)\n",
    "\n",
    "• She stood up slowly (is a 4-gram)\n",
    "\n",
    "We can use the **ngram** function in **nltk** to identify the most frequent n-grams in the text file we just created\n",
    "\n",
    "You will see on line 3 in the code we import 'stopwords'. This is a list of the most common words that cwe wish to exclude with any analysis.\n",
    "\n",
    "This code will obtain the most common 2 word sequences.\n",
    "\n",
    "The number of words (N) is set to 2 with the following line in the code:\n",
    "\n",
    "`N = 2`\n",
    "\n",
    "Change this to another number to experiment with different N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams  # function for making ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "source_file = open(\"tweets.txt\")  # open file\n",
    "txt = source_file.read()  # add file contents to variable\n",
    "txt = txt.lower()  # lower case text\n",
    "\n",
    "stop_words = stopwords.words('english') \n",
    "\n",
    "N = 2\n",
    "\n",
    "# Apply the stopwords to the text\n",
    "txt = [word for word in txt.split() if word.lower() not in stop_words]\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "pairs = ngrams(txt, N) # Change this number to see different n-grams\n",
    "\n",
    "# get the frequency of each bigram in the text\n",
    "pairsFreq = collections.Counter(pairs)\n",
    "\n",
    "for k, v in pairsFreq.most_common(20): # Change this number to change the number of n-grams\n",
    "    k = ' '.join(k)\n",
    "    print(k, v)\n",
    "\n",
    "source_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "\n",
    "This is a useful technique for finding the context of all instances of a particular word in the document. \n",
    "\n",
    "This example uses the word 'nasty' as an example but you can change this in the last line of the code block below.\n",
    "\n",
    "You can also change the number of characters shown (width) and the number of lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "input_file=open('tweets.txt').read()\n",
    "\n",
    "tokens=nltk.word_tokenize(input_file)\n",
    "\n",
    "text=nltk.Text(tokens)\n",
    "\n",
    "\n",
    "print(text.concordance('nasty', width=80, lines=25)) # Experiment by exchanging the word 'nasty' for another term.\n",
    "\n",
    "# The 'width' controls the number of characters shown while the 'lines' controls the number of lines shown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "We can use the Python module  **textblob** to analyze the original CSV file. We will\n",
    "\n",
    "• Open 'tweets.csv'\n",
    "\n",
    "• Write some new header rows to label the additional information we will add\n",
    "\n",
    "• Analyze each row and assign a sentiment (positive or negative) and a polarity score (within the range -1 to 1)\n",
    "\n",
    "• Write the results to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "\n",
    "in_file = \"tweets.csv\"\n",
    "\n",
    "out_file = \"trump-tweet-sentiment.csv\"\n",
    "\n",
    "# Create file to write our results to\n",
    "sntTweets = csv.writer(open(out_file, \"w\", newline='', encoding='utf-8'))\n",
    "\n",
    "# Add column titles to the first row\n",
    "sntTweets.writerow(['Tweet ID', 'Created', 'Tweet Text', 'sentiment','polarity' ])\n",
    "\n",
    "# Open our tweets csv file\n",
    "with open(in_file,  mode='r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader, None)  # skip the existing headers\n",
    "    tweetcount = 1;  # establish a counter\n",
    "    for row in reader:\n",
    "        if ('RT @' not in row[2]): # Exclude retweets\n",
    "            tweet_id = row[0]\n",
    "            created_at = row[1]\n",
    "            tweet_text = row[2]\n",
    "\n",
    "            blob = TextBlob(tweet_text) #pass the tweet text to Textblob\n",
    "\n",
    "            polarity = (blob.sentiment.polarity) #get a polarity score\n",
    "\n",
    "            # Get the overall sentiment\n",
    "            if polarity > 0:\n",
    "              sentiment = \"positive\"\n",
    "            elif polarity < 0:\n",
    "              sentiment = \"negative\"\n",
    "            elif polarity == 0.0:\n",
    "              sentiment = \"neutral\"\n",
    "\n",
    "            #print(\"Tweet \" + str(tweetcount) + \" is \" + sentiment)\n",
    "            tweetcount = tweetcount + 1\n",
    "\n",
    "            #write data to CSV file\n",
    "            sntTweets.writerow(\n",
    "                [tweet_id, created_at, tweet_text, sentiment, polarity])\n",
    "\n",
    "    print (str(tweetcount) + ' tweets analysed for sentiment - results written to ' + out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentiment by Keyword\n",
    "\n",
    "The following code will search our new CSV file and return all instances of a keyword followed by the aggregate sentiment.\n",
    "\n",
    "Run the code block and enter a keyword when prompted then press Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "searchterm = input('Type a keyword to search for: ')\n",
    "keyword = ' ' + searchterm.lower() + ' '\n",
    "inputfile = 'trump-tweet-sentiment.csv'\n",
    "\n",
    "with open(inputfile, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile, delimiter=',')\n",
    "    next(reader, None)  # skip the existing headers\n",
    "    cnt = 0\n",
    "    polarityscore = 0\n",
    "    for row in reader:\n",
    "\n",
    "        tweet_text = row[2]\n",
    "\n",
    "        if keyword in tweet_text.lower():\n",
    "\n",
    "            blob = TextBlob(tweet_text)\n",
    "\n",
    "            polarity = (blob.sentiment.polarity)\n",
    "\n",
    "            if polarity > 0:\n",
    "                sentiment = \"positive\"\n",
    "            elif polarity < 0:\n",
    "                sentiment = \"negative\"\n",
    "            elif polarity == 0.0:\n",
    "                sentiment = \"neutral\"\n",
    "            cnt += 1\n",
    "            polarityscore = polarityscore + polarity\n",
    "\n",
    "            print(str(cnt) + \". \" + tweet_text.lower().replace(keyword,\"[\" + searchterm + \"]\") + ' [' + sentiment + ']' )\n",
    "\n",
    "if cnt > 0:\n",
    "    avgpolarity = (polarityscore / cnt)\n",
    "\n",
    "    if avgpolarity > 0:\n",
    "        avgsentiment = \"positive\"\n",
    "    elif avgpolarity < 0:\n",
    "        avgsentiment = \"negative\"\n",
    "    elif avgpolarity == 0.0:\n",
    "        avgsentiment = \"neutral\"\n",
    "\n",
    "    print ('==================================================')\n",
    "    print ( str(cnt) + ' occurences of \"' + searchterm.lstrip() + '\" found in text')\n",
    "    print ('Average Sentiment: ' + str(avgsentiment))\n",
    "    print ('Average Polarity: ' + str(round(avgpolarity,3)))\n",
    "    print ('==================================================')\n",
    "\n",
    "else:\n",
    "    print ('==================================================')\n",
    "    print ('No occurences of ' + searchterm + ' found in text')\n",
    "    print ('==================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of a text file\n",
    "\n",
    "You can also perform a similar sentiment analysis on a text file. For this example we will use the text of 'The Origin of Species' by Charles Darwin. Download this file from the following link (right-click the link then 'save link as', 'save page as' or 'download linked file' depending on your browser):\n",
    "\n",
    "[https://learn.edina.ac.uk/inter-ta/files/darwin-origin.txt](https://learn.edina.ac.uk/inter-ta/files/darwin-origin.txt)\n",
    "\n",
    "\n",
    "Once you have downloaded the file, go back to the Noteable Home tab and upload to Noteable in the same way you uploaded this Notebook. \n",
    "\n",
    "There are other source text files that you can use here:\n",
    "\n",
    "[https://learn.edina.ac.uk/inter-ta](https://learn.edina.ac.uk/inter-ta)\n",
    "\n",
    "Or you can upload your own files.\n",
    "\n",
    "Once you have uploaded a different file, change the filename in the 4th line of the following code block to reflect this.\n",
    "\n",
    "When you run this code it will prompt you for a keyword. Enter a keyword and hit 'Return'. It will return the average sentiment for that keyword, as well as the text of the most negative and positive occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "source_text = 'darwin-origin.txt'\n",
    "\n",
    "source_file = open(source_text) # open file\n",
    "\n",
    "txt = source_file.read()     # add file contents to variable\n",
    "\n",
    "tokenized_text=sent_tokenize(txt) # Tokenize text into sentences\n",
    "\n",
    "searchterm = input('Please enter a search term: ')\n",
    "\n",
    "cnt =0\n",
    "polarityscore=0\n",
    "\n",
    "# Establish variables to hold highest and lowest polarity scores and the sentences they refer to.\n",
    "hipol=0\n",
    "lopol=0\n",
    "hitext=''\n",
    "lotext=''\n",
    "\n",
    "# Loop through each sentence\n",
    "for s in tokenized_text:\n",
    "    if searchterm.lower() in s.lower(): # transform the searchterm\n",
    "        blob = TextBlob(s)  # pass the tweet text to Textblob\n",
    "\n",
    "        polarity = (blob.sentiment.polarity)  # get a polarity score\n",
    "\n",
    "        polarityscore=polarityscore + polarity # add polarity score to overall polarity total\n",
    "        if polarity > hipol:\n",
    "            hipol = polarity\n",
    "            hitext = s\n",
    "\n",
    "        if polarity < lopol:\n",
    "            lopol = polarity\n",
    "            lotext = s\n",
    "\n",
    "        cnt +=1\n",
    "\n",
    "if hipol == 0:\n",
    "    hitext = 'No text containing the keyword is positive'\n",
    "\n",
    "if lopol == 0:\n",
    "    lotext = 'No text containing the keyword is negative'\n",
    "\n",
    "if cnt > 0:\n",
    "    avgpolarity = (polarityscore / cnt) # Divide total polarity by number of sentences returned to obtain average\n",
    "\n",
    "    if avgpolarity > 0:\n",
    "        avgsentiment = \"positive\"\n",
    "    elif avgpolarity < 0:\n",
    "        avgsentiment = \"negative\"\n",
    "    elif avgpolarity == 0.0:\n",
    "        avgsentiment = \"neutral\"\n",
    "\n",
    "    print ('==================================================')\n",
    "    print ( str(cnt) + ' occurences of \"' + searchterm.lstrip() + '\" found in ' + source_text)\n",
    "    print ('Average Sentiment: ' + str(avgsentiment))\n",
    "    print ('Average Polarity: ' + str(round(avgpolarity,3)))\n",
    "    print ('--------------------------------------------------')\n",
    "    print ('Highest score: ' + str(round(hipol,3)))\n",
    "    print ('Text: ' + str(hitext))\n",
    "    print ('--------------------------------------------------')\n",
    "    print ('Lowest score:' + str(round(lopol,3)))\n",
    "    print ('Text: ' + str(lotext))\n",
    "    print ('==================================================')\n",
    "\n",
    "\n",
    "else:\n",
    "    print ('==================================================')\n",
    "    print ('No occurences of ' + searchterm + ' found in text')\n",
    "    print ('==================================================')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a document or collection of documents.\n",
    "\n",
    "The following example will attempt to identifiy the most common topics in a document, as well as providing snippets of text that illustrate these topics. (View the file 'processed-darwin-origin.txt' in the homw tab to view the snippets)\n",
    "\n",
    "As before you can replace the file name with an alternative once you have uploaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess a document using scikit-learn\n",
    "#\n",
    "# Load doc into a list and create a short snippet of text for each document.\n",
    "import os\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "source_file = 'darwin-origin.txt'\n",
    "\n",
    "out_file = 'processed-' + source_file\n",
    "f = open(out_file,'w')\n",
    "\n",
    "raw_documents = []\n",
    "snippets = []\n",
    "\n",
    "with open(source_file ,\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        raw_documents.append( text )\n",
    "        # keep a short snippet of up to 100 characters as a title for each document\n",
    "        snippets.append( text[0:min(len(text),100)] )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))\n",
    "\n",
    "stop_words = stopwords.words('english') \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, min_df = 20)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "\n",
    "print('Topic Analysis of ' + source_file)\n",
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "import operator\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "ranking = rank_terms( A, terms )\n",
    "k = 10\n",
    "\n",
    "# create the model\n",
    "from sklearn import decomposition\n",
    "model = decomposition.NMF( init=\"nndsvd\", n_components=k )\n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform( A )\n",
    "H = model.components_\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"       10 Most Prominent Topics \")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "f.write(\"-------------------------------------------\\n\")\n",
    "f.write(\"       10 Most Prominent Topics \\n\")\n",
    "f.write(\"-------------------------------------------\\n\")\n",
    "\n",
    "# get a descriptor for each topic using the top ranked terms (e.g. top 10):\n",
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    f.write(\"Topic %02d: %s\" % (topic_index + 1, str_descriptor) +\"\\n\")\n",
    "    print(\"Topic %02d: %s\" % (topic_index + 1, str_descriptor))\n",
    "# get snippets for each topic\n",
    "\n",
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    # now get the snippets corresponding to the top-ranked indices\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets\n",
    "\n",
    "cnt = 0\n",
    "for topic_index in range(k):\n",
    "    t = cnt+1\n",
    "    f.write(\"\\n-------------------------------------------\\n\")\n",
    "    f.write(\"         Topic \"  + str(t ) + \". Snippets\")\n",
    "    f.write(\"\\n-------------------------------------------\\n\")\n",
    "    topic_snippets = get_top_snippets( snippets, W, cnt, 10 )\n",
    "    for i, snippet in enumerate(topic_snippets):\n",
    "      f.write(\"%02d. %s\" % ( (i+1), snippet ) +\"\\n\")\n",
    "\n",
    "    cnt = cnt +1\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print(\"  Topics written to \" + out_file )\n",
    "print (\"-------------------------------------------\")\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "The following code finds the most frequent terms in a text file and produce a **wordcloud**\n",
    "\n",
    "As before you can change the source file to one of your own choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import wordcloud, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words= stopwords.words(\"english\") + [' ', '\\n','``','\\'s.','\\'\\'','n\\'t','\\'s', 'https',\n",
    "                                                   '@realdonaldtrump', 'realdonaldtrump','--','http',\n",
    "                                                   'rt', '@potus', 'amp', 'trump', '...']\n",
    "\n",
    "source_file = open(\"tweets.txt\") # open file\n",
    "txt = source_file.read()          # add file contents to variable\n",
    "txt = txt.lower()  # lower case text\n",
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
    "\n",
    "tokenized_txt=word_tokenize(txt)\n",
    "\n",
    "filtered_words=[] # Create an empty list\n",
    "\n",
    "for w in tokenized_txt:\n",
    "    if w not in stop_words: # Remove common words\n",
    "        filtered_words.append(w) # Append word to list\n",
    "        \n",
    "\n",
    "txt = ' '.join(filtered_words).lower() # Join words back together\n",
    "\n",
    "out_file = open(\"text.txt\",\"w\") \n",
    "\n",
    "out_file.write(txt)\n",
    "\n",
    "\"\"\"\n",
    "Generating a  wordcloud from the input text.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Read the whole text.\n",
    "text = open('text.txt').read()\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wordcloud = WordCloud(width=480, height=240, max_font_size=80,colormap=\"Greens\", min_font_size=10).generate(text)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency Barchart\n",
    "\n",
    "A similar approach, this time the frequencies are outputted to a barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "source_file = open(\"darwin-origin.txt\") # open file\n",
    "txt = source_file.read()          # add file contents to variable\n",
    "\n",
    "\n",
    "txt = txt.lower()  # lower case text\n",
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
    "\n",
    "all_words=word_tokenize(txt)\n",
    "\n",
    "\n",
    "filtered_word=[]\n",
    "for w in all_words:\n",
    "    if w not in stop_words:\n",
    "        filtered_word.append(w)\n",
    "\n",
    "counts = dict(Counter(filtered_word).most_common(10))\n",
    "\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "# rearrange your data\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10,8)) # change figsize to (width, height), to the size you want\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# add labels\n",
    "plt.xticks(indexes + bar_width, labels)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Plots\n",
    "\n",
    "This is a technique for visualizing where particular terms appear in a text, e.g. Is the term found consistently throughout a text or does it tend to be found in one area.\n",
    "\n",
    "If we use the tweets.txt file, because the tweets are listed chronologically, we can get an impression of how common certain terms are at particular times.\n",
    "\n",
    "Two terms are specified in the last line of the code but more can be entered. Change the terms as appropriate to your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "input_file=open('tweets.txt').read()\n",
    "\n",
    "tokens=nltk.word_tokenize(input_file)\n",
    "\n",
    "\n",
    "text=nltk.Text(tokens)\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # change figsize to (width, height), to the size you want\n",
    "\n",
    "\n",
    "text.dispersion_plot([\"Obama\",\"Hillary\",\"Biden\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exercises\n",
    "\n",
    "Experiment by analysing different text files. A selection can be found here (or use a file of you own choosing):\n",
    "\n",
    "[https://learn.edina.ac.uk/inter-ta](https://learn.edina.ac.uk/inter-ta)\n",
    "\n",
    "Once the file has been saved to your computer, go back to the Noteable home tab in the browser.\n",
    "\n",
    "* Select 'Upload' from the top right of the page. \n",
    "* Browse to the file.\n",
    "* Click 'Select'\n",
    "* Click on the blue 'Upload' button\n",
    "\n",
    "The file is now available to be used in Noteable.\n",
    "\n",
    "In the code blocks replace the original filename with the name of your file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
